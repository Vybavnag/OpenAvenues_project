{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import threading\n",
    "import time\n",
    "from queue import Queue\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "class Message:\n",
    "    def __init__(self, timestamp: int, userId: str, type: str, product_Id: str, brand: str, category: str):\n",
    "        self.timestamp = timestamp\n",
    "        self.userId = userId\n",
    "        self.type = type\n",
    "        self.product_Id = product_Id\n",
    "        self.brand = brand\n",
    "        self.category = category\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Message:\\n\\tUser Id: {self.userId}\\n\\tAt Timestamp: {self.timestamp}\\n\\tType: {self.type}\\n\\tProduct Id: {self.product_Id}\\n\\tBrand: {self.brand}\\n\\tCategory: {self.category}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class KafkaBroker:\n",
    "    def __init__(self):\n",
    "        self.message_queues = {}\n",
    "\n",
    "    def produce_message(self, message: Message, topic: str) -> None:\n",
    "        if topic not in self.message_queues:\n",
    "            self.message_queues[topic] = Queue()\n",
    "        self.message_queues[topic].put(message)\n",
    "\n",
    "    def consume_messages(self):\n",
    "        while True:\n",
    "            messages_exist = False\n",
    "            for index, message_queue in self.message_queues.items():\n",
    "                if not message_queue.empty():\n",
    "                    messages_exist = True\n",
    "                    yield message_queue.get()\n",
    "            if not messages_exist:\n",
    "                time.sleep(0.01)\n",
    "                continue\n",
    "            yield None  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "KafkaProducer represents a message producer that sends messages to a topic.\n",
    "\"\"\"\n",
    "class KafkaProducer:\n",
    "    def __init__(self, broker: KafkaBroker, topic: str, data: pd.DataFrame, name: str):\n",
    "        self.broker = broker\n",
    "        self.topic = topic\n",
    "        self.data = data\n",
    "        self.thread = None\n",
    "        self.name = name  \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Starts the producer thread.\n",
    "    \"\"\"\n",
    "    def start(self):\n",
    "        self.thread = threading.Thread(target=self.produce_messages)\n",
    "        self.thread.start()\n",
    "        \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Sends a message to the broker.\n",
    "    \"\"\"\n",
    "    def send(self, message: Message):\n",
    "        #print(f\"Sending message from {self.name}: {message}\")\n",
    "        self.broker.produce_message(message, self.topic)\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Randomly samples messages from the data and sends them to the broker.\n",
    "    \"\"\"\n",
    "    def produce_messages(self):\n",
    "        df = self.data.sample(10000)\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            timestamp = int(pd.Timestamp(row['event_time']).timestamp())\n",
    "            user_id = str(row['user_id'])\n",
    "            event_type = str(row['event_type'])\n",
    "            product_id = row['product_id']\n",
    "            brand = row['brand']\n",
    "            category = row['category_code']\n",
    "            message = Message(timestamp, user_id, event_type, product_id, brand, category)\n",
    "            self.send(message)\n",
    "            time.sleep(0.001)\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "KafkaConsumer represents a message consumer that listens to messages from a topic.\n",
    "\"\"\"\n",
    "class KafkaConsumer:\n",
    "    def __init__(self, broker: KafkaBroker, topic: str, layer_type: str, name: str):\n",
    "        self.broker = broker\n",
    "        self.topic = topic\n",
    "        self.queue = Queue()\n",
    "        self.thread = None\n",
    "        self.layer_type = layer_type\n",
    "        self.name = name  \n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Starts the consumer thread.\n",
    "    \"\"\"\n",
    "    def start(self):\n",
    "        self.thread = threading.Thread(target=self.run)\n",
    "        self.thread.start()\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Listens to the broker and collects incoming messages.\n",
    "    \"\"\"\n",
    "    def run(self):\n",
    "        for message in self.broker.consume_messages():\n",
    "            if message is not None:\n",
    "                self.queue.put(message)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Fetches a message from the queue.\n",
    "    \"\"\"\n",
    "    def poll(self, timeout=1):\n",
    "        try:\n",
    "            return self.queue.get(timeout=timeout)\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "BatchLayer processes the incoming messages in a batch manner and computes the trending products.\n",
    "\"\"\"\n",
    "class BatchLayer:\n",
    "    def __init__(self, start_date, end_date):\n",
    "        self.popularity = {}\n",
    "        self.buckets = {}\n",
    "        self.user_interactions = defaultdict(set)\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Computes trending products based on their popularity change.\n",
    "    \"\"\"\n",
    "    def precompute_trending(self, min_change_threshold=10):\n",
    "        trending_items = {}\n",
    "        if len(self.buckets) >= 2:\n",
    "            for bucket in range(len(self.buckets) - 1):\n",
    "                current_bucket = self.buckets.get(bucket, {})\n",
    "                next_bucket = self.buckets.get(bucket + 1, {})\n",
    "                trending_items[bucket] = {\n",
    "                    product: ((next_bucket.get(product, 0) - current_bucket.get(product, 0)) / (current_bucket.get(product, 0) + next_bucket.get(product, 0) + 1))\n",
    "                    for product in set(list(current_bucket.keys()) + list(next_bucket.keys()))\n",
    "                    if abs((next_bucket.get(product, 0) - current_bucket.get(product, 0))) >= min_change_threshold\n",
    "                }\n",
    "                print(f\"Bucket {bucket}: Trending items: {trending_items[bucket]}\")\n",
    "\n",
    "            trending_flat = {}\n",
    "            for items in trending_items.values():\n",
    "                trending_flat.update(items)\n",
    "\n",
    "            top_trending_items = dict(sorted(trending_flat.items(), key=lambda item: item[1], reverse=True)[:5])\n",
    "            product_ids = list(top_trending_items.keys())\n",
    "            trending_percentages = list(top_trending_items.values())\n",
    "\n",
    "            # Adjusted the trending percentage condition\n",
    "            trending_percentages = [percentage if abs(percentage) > 1.0 else 1.0 for percentage in trending_percentages]\n",
    "\n",
    "            return product_ids, trending_percentages\n",
    "        else:\n",
    "            print(\"Insufficient data to compute trending.\")\n",
    "            return [], []\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Processes each incoming message to update product popularity and user interactions.\n",
    "    \"\"\"\n",
    "    def process_realtime_event(self, message):\n",
    "        event_time = pd.to_datetime(message.timestamp, unit='s')\n",
    "        event_type = message.type\n",
    "        product_id = message.product_Id\n",
    "        user_id = message.userId\n",
    "\n",
    "        if event_type in ['view', 'cart']:\n",
    "            self.user_interactions[user_id].add(product_id)\n",
    "\n",
    "        if event_type not in ['view', 'cart']:\n",
    "            if self.start_date <= event_time.date() <= self.end_date:\n",
    "                # Update popularity count for the product\n",
    "                self.popularity[product_id] = self.popularity.get(product_id, 0) + 1\n",
    "\n",
    "                # Update popularity for each product in the current bucket\n",
    "                bucket_start = (event_time.date() - self.start_date).days // 7\n",
    "                self.buckets.setdefault(bucket_start, {})\n",
    "                for prod_id in self.popularity:\n",
    "                    self.buckets[bucket_start][prod_id] = self.popularity[prod_id]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "SpeedLayer processes the incoming messages in real-time to find similar products based on user interactions.\n",
    "\"\"\"\n",
    "class SpeedLayer:\n",
    "    def __init__(self, max_interactions=5):\n",
    "        self.max_interactions = max_interactions\n",
    "        self.user_interactions = defaultdict(list)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Processes each incoming message to update recent user interactions.\n",
    "    \"\"\"\n",
    "    def process_realtime_event(self, message):\n",
    "        product_id = message.product_Id\n",
    "        brand = message.brand\n",
    "        category = message.category\n",
    "\n",
    "        user_id = message.userId\n",
    "        if user_id not in self.user_interactions:\n",
    "            self.user_interactions[user_id] = [(product_id, brand, category)]\n",
    "        else:\n",
    "            self.user_interactions[user_id].append((product_id, brand, category))\n",
    "            if len(self.user_interactions[user_id]) > self.max_interactions:\n",
    "                self.user_interactions[user_id] = self.user_interactions[user_id][-self.max_interactions:]\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Finds products similar to those that a specific user interacted with.\n",
    "    \"\"\"\n",
    "    def find_similar_products(self, user_id, num_similar=5):\n",
    "        user_interactions = set(self.user_interactions.get(user_id, []))\n",
    "        similar_products = defaultdict(float)\n",
    "        for u_id, interactions in self.user_interactions.items():\n",
    "            if u_id != user_id:\n",
    "                interactions_set = set(interactions)\n",
    "                common_interactions = len(interactions_set.intersection(user_interactions))\n",
    "                similarity_score = common_interactions / (len(user_interactions) + len(interactions_set) - common_interactions)\n",
    "                for product, _, _ in interactions_set:\n",
    "                    similar_products[product] += similarity_score\n",
    "\n",
    "        similar_products = dict(sorted(similar_products.items(), key=lambda item: item[1], reverse=True))\n",
    "        return list(similar_products.keys())[:num_similar]\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "ServingLayer stores the precomputed results and provides an interface to access these results.\n",
    "\"\"\"\n",
    "class ServingLayer:\n",
    "    def __init__(self):\n",
    "        self.precomputed_results = []\n",
    "\n",
    "    def store_precomputed_results(self, product_ids, trending_percentages):\n",
    "        self.precomputed_results = list(zip(product_ids, trending_percentages))\n",
    "        print(\"Precomputed results stored.\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.precomputed_results)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        data = pd.read_csv('2019-Nov.csv')\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: The file '2019-Nov.csv' was not found.\")\n",
    "        exit()\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The file '2019-Nov.csv' is empty.\")\n",
    "        exit()\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error: There was an issue parsing the file '2019-Nov.csv'.\")\n",
    "        exit()\n",
    "\n",
    "    broker = KafkaBroker()\n",
    "    consumer_batch = KafkaConsumer(broker, 'user_activities', layer_type='Batch', name='Consumer Batch')\n",
    "    consumer_speed = KafkaConsumer(broker, 'user_activities', layer_type='Speed', name='Consumer Speed')\n",
    "    producer = KafkaProducer(broker, 'user_activities', data, name='Producer')\n",
    "\n",
    "    producer.start()\n",
    "    consumer_batch.start()\n",
    "    consumer_speed.start()\n",
    "\n",
    "    batch_layer = BatchLayer(start_date=datetime.date(2019, 11, 1), end_date=datetime.date(2019, 11, 30))\n",
    "    speed_layer = SpeedLayer()\n",
    "\n",
    "    timeout_seconds = 20\n",
    "    consumed_messages = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        message_batch = consumer_batch.poll()\n",
    "        message_speed = consumer_speed.poll()\n",
    "\n",
    "        if message_batch:\n",
    "            consumed_messages += 1\n",
    "            print(f\"[#{consumed_messages}] {consumer_batch.name} received: {message_batch}\")\n",
    "            batch_layer.process_realtime_event(message_batch)\n",
    "        if message_speed:\n",
    "            consumed_messages += 1\n",
    "            print(f\"[#{consumed_messages}] {consumer_speed.name} received: {message_speed}\")\n",
    "            speed_layer.process_realtime_event(message_speed)\n",
    "\n",
    "        if not message_batch and not message_speed and time.time() - start_time >= timeout_seconds:\n",
    "            break\n",
    "\n",
    "        time.sleep(0.01)\n",
    "\n",
    "\n",
    "    print(\"Batch Layer Results:\")\n",
    "    min_change_threshold = 1\n",
    "    product_ids, trending_percentages = batch_layer.precompute_trending(min_change_threshold)\n",
    "    print(\"Product IDs:\", product_ids)\n",
    "    print(\"Trending Percentages:\", trending_percentages)\n",
    "\n",
    "    print(\"Speed Layer Results:\")\n",
    "    user_id = '532647354'\n",
    "    similar_products = speed_layer.find_similar_products(user_id, num_similar=5)\n",
    "    print(\"Similar Product Recommendations for User {}: {}\".format(user_id, similar_products))\n",
    "\n",
    "    serving_layer = ServingLayer()\n",
    "\n",
    "    serving_layer.store_precomputed_results(product_ids, trending_percentages)\n",
    "\n",
    "    print(\"Serving Layer Results:\")\n",
    "    stored_results = serving_layer.__str__()\n",
    "    print(\"Stored Results:\", stored_results)\n",
    "\n",
    "    # Extracting the product with the highest trending percentage\n",
    "    highest_trending_product = None\n",
    "    highest_trending_percentage = 0\n",
    "    for product_id, trending_percentage in zip(product_ids, trending_percentages):\n",
    "        if trending_percentage > highest_trending_percentage:\n",
    "            highest_trending_product = product_id\n",
    "            highest_trending_percentage = trending_percentage\n",
    "    print(\"Product with the Highest Trending Percentage:\", highest_trending_product)\n",
    "\n",
    "    # Extracting the most similar product from the speed layer\n",
    "    most_similar_product = similar_products[0]\n",
    "    print(\"Most Similar Product from Speed Layer:\", most_similar_product)\n",
    "\n",
    "    # Additional recommendation based on results\n",
    "    most_similar_product_data = data[data['product_id'] == most_similar_product].iloc[0]\n",
    "    highest_trending_product_data = data[data['product_id'] == highest_trending_product].iloc[0]\n",
    "    print(f\"Recommending [Product ID: {highest_trending_product}, Category: {highest_trending_product_data['category_code']}, Brand: {highest_trending_product_data['brand']} and Product ID: {most_similar_product}, Category: {most_similar_product_data['category_code']}, Brand: {most_similar_product_data['brand']}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
